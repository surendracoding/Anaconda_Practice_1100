{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############IMP\n",
    "#1. SparkSession: Main Entry point\n",
    "sparkSession_Var = SparkSession \\\n",
    "       .builder \\\n",
    "       .appName(\"RandomForest\") \\\n",
    "       .config(\"spark.executor.heartbeatInterval\",\"60s\")\\\n",
    "       .getOrCreate()\n",
    "\n",
    "    sc = spark.sparkContext\n",
    "    sqlContext = SQLContext(sc)\n",
    "\n",
    "What is SparkContext?\n",
    "The driver program use the SparkContext to connect and communicate with the cluster and it helps in executing and coordinating the Spark job with the resource managers like YARN or Mesos.\n",
    "Using SparkContext you can actually get access to other contexts like  SQLContext and HiveContext.\n",
    "Using SparkContext we can set configuration parameters to the Spark job.\n",
    "If you are in spark-shell, a SparkContext is already available for you and is assigned to the variable sc. If you don’t have a SparkContext already, you can create one by first creating a SparkConf first.\n",
    "\n",
    "//set up the spark configuration\n",
    "val sparkConf = new SparkConf().setAppName(\"hirw\").setMaster(\"yarn\")\n",
    "//get SparkContext using the SparkConf\n",
    "val sc = new SparkContext(sparkConf)\n",
    "\n",
    "What is a SQLContext?\n",
    "SQLContext is your gateway to SparkSQL. Here is how you create a SQLContext using the SparkContext.\n",
    "\n",
    "// sc is an existing SparkContext.\n",
    "val sqlContext = new org.apache.spark.sql.SQLContext(sc)\n",
    "\n",
    "Once you have the SQLContext you can start working with DataFrame, DataSet etc.\n",
    "\n",
    "What is a HiveContext?\n",
    "HiveContext is your gateway to Hive. HiveContext has all the functionalities of a SQLContext. In fact, if you look at the API documentation you can see that HiveContext extends SQLContext, meaning, it has support the functionalities that SQLContext support plus more (Hive specific functionalities)\n",
    "\n",
    "public class HiveContext\n",
    "extends SQLContext\n",
    "implements Logging\n",
    "\n",
    "Here is how we can get a HiveContext using the SparkContext\n",
    "\n",
    "// sc is an existing SparkContext.\n",
    "val sqlContext = new org.apache.spark.sql.hive.HiveContext(sc)\n",
    "\n",
    "What is a SparkSession?\n",
    "SparkSession was introduced in Spark 2.0 to make it easy for the developers so we don’t have worry about different contexts and to streamline the access to different contexts. By having access to SparkSession, we automatically have access to the SparkContext.\n",
    "\n",
    "Here is how we can create a SparkSession –\n",
    "\n",
    "val spark = SparkSession\n",
    ".builder()\n",
    ".appName(\"hirw-test\")\n",
    ".config(\"spark.some.config.option\", \"some-value\")\n",
    ".getOrCreate()\n",
    "\n",
    "SparkSession is now the new entry point of Spark that replaces the old SQLContext and HiveContext. Note that the old SQLContext and HiveContext are kept for backward compatibility.\n",
    "\n",
    "Once we have access to a SparkSession, we can start working with DataFrame and Dataset. Simply using the SparkSession to read a JSON file in to a DataFrame.\n",
    "\n",
    "val df = spark.read.json(\"path/to/file.json\")\n",
    "\n",
    "Here is how we create a SparkSession with Hive support.\n",
    "\n",
    "val spark = SparkSession\n",
    ".builder()\n",
    ".appName(\"hirw-hive-test\")\n",
    ".config(\"spark.sql.warehouse.dir\", warehouseLocation)\n",
    ".enableHiveSupport()\n",
    ".getOrCreate()\n",
    "\n",
    "So if you are using Spark 2.0 and above, you will use SparkSession."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################Spark ######################\n",
    "#1. Apache Spark is an open source cluster computing framework that provides an interface for entire programming clusters with implicit data parallelism and fault-tolerance.\n",
    "#2. Apache Spark is devised to serve as a general-purpose and fast cluster computing platform.\n",
    "\n",
    "######################Spark Conf######################\n",
    "#1. SparkConf stores configuration parameters for a Spark application.\n",
    "#2. These configuration parameters can be properties of the Spark driver application or utilized by Spark to allot resources on the cluster, like memory size and cores.\n",
    "#3. SparkConf object can be created with new SparkConf() and permits you to configure standard properties and arbitrary key-value pairs via the set() method.\n",
    "#4. EX:\n",
    "    val conf = new SparkConf().setMaster(\"local[4]\").setAppName(\"FirstSparkApp\")\n",
    "    val sc = new SparkContext(conf)\n",
    "    \n",
    "    \n",
    "###################### Spark Context ######################\n",
    "#1. Main entry point for Spark functionality\n",
    "#2. SparkContext can be utilized to create broadcast variables, RDDs, and accumulators, and denotes the connection to a Spark cluster.\n",
    "#3. To create a SparkContext, you first have to develop a SparkConf object that includes details about your application.\n",
    "#4. As shown in the diagram, the Spark driver program uses SparkContext to connect to the cluster manager for resource allocation, submit Spark jobs and knows what resource manager (YARN, Mesos or Standalone) to communicate.\n",
    "#5. Via SparkContext, the driver can access other contexts like StreamingContext, HiveContext, and SQLContext to program Spark.\n",
    "#6. There may be only one SparkContext active per JVM. Before creating a new one, you have to stop() the active SparkContext.\n",
    "#7. In the Spark shell, there is already a special interpreter-aware SparkContext created in the variable named as sc.\n",
    "#8. EX:\n",
    "     def create_sc():\n",
    "        sc_conf = SparkConf()\n",
    "        sc_conf.setAppName(\"finance-similarity-app\")\n",
    "        sc_conf.setMaster('spark://10.21.208.21:7077')\n",
    "        sc_conf.set('spark.executor.memory', '2g')\n",
    "        sc_conf.set('spark.executor.cores', '4')\n",
    "        sc_conf.set('spark.cores.max', '40')\n",
    "        sc_conf.set('spark.logConf', True)\n",
    "        print sc_conf.getAll()\n",
    "\n",
    "        sc = None\n",
    "        try:\n",
    "            sc.stop()\n",
    "            sc = SparkContext(conf=sc_conf)\n",
    "        except:\n",
    "            sc = SparkContext(conf=sc_conf)\n",
    "\n",
    "        return sc \n",
    "\n",
    "    \n",
    "#9. SparkSession:\n",
    "spark = SparkSession \\\n",
    "       .builder \\\n",
    "       .appName(\"RandomForest\") \\\n",
    "       .config(\"spark.executor.heartbeatInterval\",\"60s\")\\\n",
    "       .getOrCreate()\n",
    "\n",
    "    sc = spark.sparkContext\n",
    "    sqlContext = SQLContext(sc)\n",
    "###################### Spark Dataframe ######################    \n",
    "#1. DataFrames can be created from a wide array of sources like existing RDDs, external databases, tables in Hive, or structured data files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################Resilient Distributed Datasets (RDDs) ######################\n",
    "#Resilient distributed datasets (RDDs) are known as the main abstraction in Spark.  It is a partitioned collection of objects spread across a cluster, and can be persisted in memory or on disk. Once RDDs are created they are immutable\n",
    "data = [1, 2, 3, 4, 5]\n",
    "distData = sc.parallelize(data)\n",
    "distFile = sc.textFile(\"data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### DataFrame, Data Sources######################\n",
    "#SparkSession\n",
    ">>pyspark\n",
    "from __future__ import print_function\n",
    "from pyspark.sql import *\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.readwriter import DataFrameWriter\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()  \n",
    "spark.stop()\n",
    "exit()\n",
    "\n",
    "#Create SparkDataFrame \n",
    "from pyspark.sql import *\n",
    "Student = Row(\"firstName\", \"lastName\", \"age\", \"telephone\")\n",
    "s1 = Student('David', 'Julian', 22, 100000)\n",
    "s2 = Student('Mark', 'Webb', 23, 658545)\n",
    "StudentData=[s1,s2]\n",
    "df=spark.createDataFrame(StudentData)\n",
    "df.show()\n",
    "\n",
    "O/P:\n",
    "+---------+--------+---+---------+\n",
    "|firstName|lastName|age|telephone|\n",
    "+---------+--------+---+---------+\n",
    "|    David|  Julian| 22|   100000|\n",
    "|     Mark|    Webb| 23|   658545|\n",
    "+---------+--------+---+---------+\n",
    "\n",
    "#Generic Load/Save Functions\n",
    "df = spark.read.load(\"file path\")\n",
    "df.select(\"column name\", \"column name\").write.save(\"file name\")\n",
    "\n",
    "#Specific File Formats\n",
    "df = spark.read.load(\"path of json file\", format=\"json\")\n",
    "\n",
    "#Reading A Parquet File, For saving the dataframe into parquet format, Verifying The Result\n",
    "df = spark.read.json(\"path of the file\")\n",
    "df.write.parquet(\"parquet file name\")\n",
    "pf = spark.read.parquet(\"parquet file name\")\n",
    "\n",
    "#CSV Loading\n",
    "df = spark.read.csv(\"path-of-file/fifa_players.csv\", inferSchema = True, header = True)\n",
    "\n",
    "#How To Check The Schema\n",
    "df.printSchema()\n",
    "O/P:\n",
    "root\n",
    " |-- ID: integer (nullable = true)\n",
    " |-- Name: string (nullable = true)\n",
    " |-- Age: integer (nullable = true)\n",
    " |-- Nationality: string (nullable = true\n",
    " |-- Overall: integer (nullable = true)\n",
    " |-- Potential: integer (nullable = true)\n",
    " |-- Club: string (nullable = true)\n",
    "                                                    \n",
    "#Column Names and Count (Rows and Column)                        \n",
    "df.columns     #O/P: ['ID', 'Name', 'Age', 'Nationality', 'Overall', 'Potential', 'Club']\n",
    "df.count()\n",
    "len(df.columns)\n",
    "                          \n",
    "#Describing A Different Column\n",
    "df.describe('Age').show()\n",
    "\n",
    "O/P:\n",
    "+-------+------------------+\n",
    "|summary|               Age|\n",
    "+-------+------------------+\n",
    "|  count|             17981|\n",
    "|   mean|25.144541460430453|\n",
    "| stddev| 4.614272345005111|\n",
    "|    min|                16|\n",
    "|    max|                47|\n",
    "+-------+------------------+\n",
    "                          \n",
    "#Selecting Multiple Columns\n",
    "df.select('Column name 1,'Column name 2',......,'Column name n').show()\n",
    "dfnew=df.select('ID','Name')\n",
    "dfnew.show(5)\n",
    "O/P:\n",
    "+------+-----------------+\n",
    "|    ID|             Name|\n",
    "+------+-----------------+\n",
    "| 20801|Cristiano Ronaldo|\n",
    "|158023|         L. Messi|\n",
    "|190871|           Neymar|\n",
    "|176580|        L. Suárez|\n",
    "|167495|         M. Neuer|\n",
    "+------+-----------------+          \n",
    "          \n",
    "          \n",
    "#Filtering Data          \n",
    "df.filter(df.Club=='FC Barcelona').show(3)\n",
    "df.filter((df.Club=='FC Barcelona') & (df.Nationality=='Spain')).show(3)\n",
    "          \n",
    "#Sorting Data (OrderBy)\n",
    "df.filter((df.Club=='FC Barcelona') & (df.Nationality=='Spain')).orderBy('ID',).show(5)\n",
    "df.filter((df.Club=='FC Barcelona') & (df.Nationality=='Spain')).orderBy('ID',ascending=False).show(5) \n",
    "          \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### HIVE HDFS Spark ######################\n",
    "#what is HIVE: Apache Hive is built on top of Apache Hadoop. The Apache Hive data warehouse software allows reading, writing, and managing large datasets residing in distributed storage and queried using SQL syntax.\n",
    "#How To Enable Hive Support\n",
    "from os.path import expanduser, join, abspath\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "\n",
    "#warehouse_location points to the default location for managed databases and tables\n",
    "warehouse_location = abspath('spark-warehouse')\n",
    "\n",
    "#SparkSession starts\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL Hive integration example\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", warehouse_location) \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "#Creating Hive Table From Spark\n",
    "spark.sql(\"\"\"CREATE TABLE IF NOT EXISTS \n",
    "              table_name(column_name_1 DataType,column_name_2 DataType,......,column_name_n DataType) \n",
    "          USING hive\"\"\")\n",
    "\n",
    "#To load a DataFrame into a table.\n",
    "df.write.insertInto(\"our_table_name\", overwrite= True)\n",
    "\n",
    "#Handling External Hive Tables From Apache Spark query in hive_shell\n",
    "hive> create external table table_name(column_name1 DataType,column_name2 DataType,......,column_name_n DataType) STORED AS Parquet location ' path of external table';\n",
    "\n",
    "#Loading Data From Spark To The Hive Table\n",
    "df = spark.read.csv(\"path-of-file\", inferSchema = True, header = True)\n",
    "\n",
    "#Data Loading To External Table\n",
    "df.write.mode('overwrite').format(\"format\").save(\"location\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### HBase Spark ######################\n",
    "#What is HBase: HBase is a distributed column-oriented data store built on top of HDFS. Tables in HBase can serve as the input and output for Map Reduce jobs run in Hadoop.\n",
    "#Various Stages: It has 4 main stages which includes: Transformation, Cleaning, Validation, Writing of the data received from the various sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### Statistical and Mathematical Functions with DataFrames in Apache Spark ######################\n",
    "#Random Data Generation\n",
    "from pyspark.sql.functions import rand, randn #uniform (rand), and standard normal (randn).\n",
    "df = sqlContext.range(0, 7)\n",
    "df.show()\n",
    "\n",
    "df.select(\"id\", rand(seed=10).alias(\"uniform\"), randn(seed=27).alias(\"normal\")).show()\n",
    "O/P:\n",
    "+---+-------------------+-------------------+\n",
    "| id|            uniform|             normal|\n",
    "+---+-------------------+-------------------+\n",
    "|  0|0.41371264720975787| 0.5888539012978773|\n",
    "|  1| 0.1982919638208397|0.06157382353970104|\n",
    "|  2|0.12030715258495939| 1.0854146699817222|\n",
    "|  3|0.44292918521277047|-0.4798519469521663|\n",
    "+---+-------------------+-------------------+\n",
    "\n",
    "#Summary Statistics\n",
    "df.describe('uniform', 'normal').show()\n",
    "+-------+-------------------+--------------------+\n",
    "|summary|            uniform|              normal|\n",
    "+-------+-------------------+--------------------+\n",
    "|  count|                 10|                  10|\n",
    "|   mean| 0.3841685645682706|-0.15825812884638607|\n",
    "| stddev|0.31309395532409323|   0.963345903544872|\n",
    "|    min|0.03650707717266999| -2.1591956435415334|\n",
    "|    max| 0.8898784253886249|  1.0854146699817222|\n",
    "+-------+-------------------+--------------------+\n",
    "\n",
    "#Descriptive Statistics\n",
    "from pyspark.sql.functions import mean, min, max\n",
    "df.select([mean('uniform'), min('uniform'), max('uniform')]).show()\n",
    "O/P:\n",
    "+------------------+-------------------+------------------+\n",
    "|      avg(uniform)|       min(uniform)|      max(uniform)|\n",
    "+------------------+-------------------+------------------+\n",
    "|0.3841685645682706|0.03650707717266999|0.8898784253886249|\n",
    "+------------------+-------------------+------------------+\n",
    "\n",
    "#Sample Co-Variance \n",
    "#In statistics Co-Variance means how one random variable changes with respect to other.\n",
    "from pyspark.sql.functions import rand\n",
    "df = sqlContext.range(0, 10).withColumn('rand1', rand(seed=10)).withColumn('rand2', rand(seed=27))\n",
    "df.stat.cov('rand1', 'rand2')\n",
    "o/P:\n",
    "0.031109767020625314    \n",
    "\n",
    "#Correlation\n",
    "#Correlation provides the statistical dependence of two random variables.\n",
    "df.stat.corr('rand1', 'rand2')\n",
    "O/P:\n",
    "0.30842745432650953\n",
    "\n",
    "#Cross Tabulation (Contingency Table)\n",
    "#Cross Tabulation provides a frequency distribution table for a given set of variables.\n",
    "# Create a DataFrame with two columns (name, item)\n",
    "names = [\"Alice\", \"Bob\", \"Mike\"]\n",
    "items = [\"milk\", \"bread\", \"butter\", \"apples\", \"oranges\"]\n",
    "df = sqlContext.createDataFrame([(names[i % 3], items[i % 5]) for i in range(100)], [\"name\", \"item\"])\n",
    "df.stat.crosstab(\"name\", \"item\").show()\n",
    "O/P:\n",
    "+---------+------+-----+------+----+-------+\n",
    "|name_item|apples|bread|butter|milk|oranges|\n",
    "+---------+------+-----+------+----+-------+\n",
    "|      Bob|     6|    7|     7|   6|      7|\n",
    "|     Mike|     7|    6|     7|   7|      6|\n",
    "|    Alice|     7|    7|     6|   7|      7|\n",
    "+---------+------+-----+------+----+-------+\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transformations & Actions:\n",
    "\n",
    "map(func)\n",
    "Map transformation returns a new RDD by applying a function to each element of this RDD\n",
    "\n",
    ">>> baby_names = sc.textFile(\"baby_names.csv\")\n",
    ">>> rows = baby_names.map(lambda line: line.split(\",\"))\n",
    "So, in this transformation example, we’re creating a new RDD called “rows” by splitting every row in the baby_names RDD.  We accomplish this by mapping over every element in baby_names and passing in a lambda function to split by commas.\n",
    "\n",
    "From here, we could use Python to access the array\n",
    "\n",
    ">>> for row in rows.take(rows.count()): print(row[1])\n",
    "\n",
    "First Name\n",
    "DAVID\n",
    "JAYDEN\n",
    "...\n",
    "Back to Top\n",
    "\n",
    "\n",
    "flatMap(func)\n",
    "flatMap is similar to map, because it applies a function to all elements in a RDD.  But, flatMap flattens the results.\n",
    "\n",
    "Compare flatMap to map in the following\n",
    "\n",
    ">>> sc.parallelize([2, 3, 4]).flatMap(lambda x: [x,x,x]).collect()\n",
    "[2, 2, 2, 3, 3, 3, 4, 4, 4]\n",
    "\n",
    ">>> sc.parallelize([1,2,3]).map(lambda x: [x,x,x]).collect()\n",
    "[[1, 1, 1], [2, 2, 2], [3, 3, 3]]\n",
    "This is helpful with nested datasets such as found in JSON.\n",
    "\n",
    "Adding collect to flatMap and map results was shown for clarity.  We can focus on Spark aspect (re: the RDD return type) of the example if we don’t use collect:\n",
    "\n",
    ">>> sc.parallelize([2, 3, 4]).flatMap(lambda x: [x,x,x])\n",
    "PythonRDD[36] at RDD at PythonRDD.scala:43\n",
    "Back to Top\n",
    "\n",
    "\n",
    "filter(func)\n",
    "Create a new RDD bye returning only the elements that satisfy the search filter.  For SQL minded, think where clause.\n",
    "\n",
    ">>> rows.filter(lambda line: \"MICHAEL\" in line).collect()\n",
    "Out[36]:\n",
    "[[u'2013', u'MICHAEL', u'QUEENS', u'M', u'155'],\n",
    " [u'2013', u'MICHAEL', u'KINGS', u'M', u'146'],\n",
    " [u'2013', u'MICHAEL', u'SUFFOLK', u'M', u'142']...\n",
    "Back to Top\n",
    "\n",
    "\n",
    "mapPartitions(func, preservesPartitioning=False)\n",
    "Consider mapPartitions a tool for performance optimization if you have the resources available.  It won’t do much when running examples on your laptop.  It’s the same as “map”, but works with Spark RDD partitions which are distributed.  Remember the first D in RDD – Resilient Distributed Datasets.\n",
    "\n",
    "In examples below that when using parallelize, elements of the collection are copied to form a distributed dataset that can be operated on in parallel.\n",
    "\n",
    "A distributed dataset can be operated on in parallel.\n",
    "\n",
    "One important parameter for parallel collections is the number of partitions to cut the dataset into. Spark will run one task for each partition of the cluster.\n",
    "\n",
    ">>> one_through_9 = range(1,10)\n",
    ">>> parallel = sc.parallelize(one_through_9, 3)\n",
    ">>> def f(iterator): yield sum(iterator)\n",
    ">>> parallel.mapPartitions(f).collect()\n",
    "[6, 15, 24]\n",
    "\n",
    ">>> parallel = sc.parallelize(one_through_9)\n",
    ">>> parallel.mapPartitions(f).collect()\n",
    "[1, 2, 3, 4, 5, 6, 7, 17]\n",
    "See what’s happening?  Results [6,15,24] are created because mapPartitions loops through 3 partitions which is the second argument to the sc.parallelize call.\n",
    "\n",
    "Partion 1: 1+2+3 = 6\n",
    "\n",
    "Partition 2: 4+5+6 = 15\n",
    "\n",
    "Partition 3: 7+8+9 = 24\n",
    "\n",
    "The second example produces [1,2,3,4,5,6,7,17] which I’m guessing means the default number of partitions on my laptop is 8.\n",
    "\n",
    "Partion 1 = 1\n",
    "\n",
    "Partition 2= 2\n",
    "\n",
    "Partion 3 = 3\n",
    "\n",
    "Partition 4 = 4\n",
    "\n",
    "Partion 5 = 5\n",
    "\n",
    "Partition 6 = 6\n",
    "\n",
    "Partion 7 = 7\n",
    "\n",
    "Partition 8: 8+9 = 17\n",
    "\n",
    "Typically you want 2-4 partitions for each CPU core in your cluster. Normally, Spark tries to set the number of partitions automatically based on your cluster or hardware based on standalone environment.\n",
    "\n",
    "To find the default number of partitions and confirm the guess of 8 above:\n",
    "\n",
    ">>> print sc.defaultParallelism\n",
    "8\n",
    "Back to Top\n",
    "\n",
    "\n",
    "mapPartitionsWithIndex(func)\n",
    "Similar to mapPartitions, but also provides a function with an int value to indicate the index position of the partition.\n",
    "\n",
    ">>> parallel = sc.parallelize(range(1,10),4)\n",
    ">>> def show(index, iterator): yield 'index: '+str(index)+\" values: \"+ str(list(iterator))\n",
    ">>> parallel.mapPartitionsWithIndex(show).collect()\n",
    "\n",
    "['index: 0 values: 1',\n",
    " 'index: 1 values: 3',\n",
    " 'index: 2 values: 5',\n",
    " 'index: 3 values: 7']\n",
    "When learning these APIs on an individual laptop or desktop, it might be helpful to show differences in capabilities and outputs.  For example, if we change the above example to use a parallelized list with 3 slices, our output changes significantly:\n",
    "\n",
    ">>> parallel = sc.parallelize(range(1,10),3)\n",
    ">>> def show(index, iterator): yield 'index: '+str(index)+\" values: \"+ str(list(iterator))\n",
    ">>> parallel.mapPartitionsWithIndex(show).collect()\n",
    "\n",
    "['index: 0 values: [1, 2, 3]',\n",
    " 'index: 1 values: [4, 5, 6]',\n",
    " 'index: 2 values: [7, 8, 9]']\n",
    "Back to Top\n",
    "\n",
    "\n",
    "sample(withReplacement,fraction, seed)\n",
    "Return a random sample subset RDD of the input RDD\n",
    "\n",
    ">>> parallel = sc.parallelize(range(1,10))\n",
    ">>> parallel.sample(True,.2).count()\n",
    "2\n",
    "\n",
    ">>> parallel.sample(True,.2).count()\n",
    "1\n",
    "\n",
    ">>> parallel.sample(True,.2).count()\n",
    "2\n",
    "sample(withReplacement, fraction, seed=None)\n",
    "\n",
    "Parameters:\t\n",
    "withReplacement – can elements be sampled multiple times (replaced when sampled out)\n",
    "fraction – expected size of the sample as a fraction of this RDD’s size without replacement: probability that each element is chosen; fraction must be [0, 1] with replacement: expected number of times each element is chosen; fraction must be >= 0\n",
    "seed – seed for the random number generator\n",
    "Back to Top\n",
    "\n",
    "\n",
    "union(a different rdd)\n",
    "Simple.  Return the union of two RDDs\n",
    "\n",
    ">>> one = sc.parallelize(range(1,10))\n",
    ">>> two = sc.parallelize(range(10,21))\n",
    ">>> one.union(two).collect()\n",
    "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
    "Back to Top\n",
    "\n",
    "\n",
    "intersection(a different rdd)\n",
    "Again, simple.  Similar to union but return the intersection of two RDDs\n",
    "\n",
    ">>> one = sc.parallelize(range(1,10))\n",
    ">>> two = sc.parallelize(range(5,15))\n",
    ">>> one.intersection(two).collect()\n",
    "[5, 6, 7, 8, 9]\n",
    "Back to Top\n",
    "\n",
    "\n",
    "distinct([numTasks])\n",
    "Another simple one.  Return a new RDD with distinct elements within a source RDD\n",
    "\n",
    ">>> parallel = sc.parallelize(range(1,9))\n",
    ">>> par2 = sc.parallelize(range(5,15))\n",
    "\n",
    ">>> parallel.union(par2).distinct().collect()\n",
    "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
    "Formal API: distinct(): RDD[T]\n",
    "\n",
    "Back to Top\n",
    "\n",
    "\n",
    "The Keys\n",
    "The group of transformation functions (groupByKey, reduceByKey, aggregateByKey, sortByKey, join) all act on key,value pair RDDs.\n",
    "\n",
    "For the following, we’re going to use the baby_names.csv file again which was introduced in a previous post What is Apache Spark?\n",
    "\n",
    "All the following examples presume the baby_names.csv file has been loaded and split such as:\n",
    "\n",
    ">>> baby_names = sc.textFile(\"baby_names.csv\")\n",
    ">>> rows = baby_names.map(lambda line: line.split(\",\"))\n",
    "Back to Top\n",
    "\n",
    "\n",
    "groupByKey([numTasks])\n",
    "“When called on a dataset of (K, V) pairs, returns a dataset of (K, Iterable<V>) pairs. ”\n",
    "\n",
    "The following groups all names to counties in which they appear over the years.\n",
    "\n",
    ">>> rows = baby_names.map(lambda line: line.split(\",\"))\n",
    ">>> namesToCounties = rows.map(lambda n: (str(n[1]),str(n[2]) )).groupByKey()\n",
    ">>> namesToCounties.map(lambda x : {x[0]: list(x[1])}).collect()\n",
    "\n",
    "[{'GRIFFIN': ['ERIE',\n",
    "   'ONONDAGA',\n",
    "   'NEW YORK',\n",
    "   'ERIE',\n",
    "   'SUFFOLK',\n",
    "   'MONROE',\n",
    "   'NEW YORK',\n",
    "...\n",
    "The above example was created from baby_names.csv file which was introduced in previous post What is Apache Spark?\n",
    "\n",
    "Back to Top\n",
    "\n",
    "\n",
    "reduceByKey(func, [numTasks])\n",
    "Operates on key, value pairs again, but the func must be of type (V,V) => V\n",
    "\n",
    "Let’s sum the yearly name counts over the years in the CSV.  Notice we need to filter out the header row.  Also notice we are going to use the “Count” column value (n[4])\n",
    "\n",
    ">>> filtered_rows = baby_names.filter(lambda line: \"Count\" not in line).map(lambda line: line.split(\",\"))\n",
    ">>> filtered_rows.map(lambda n:  (str(n[1]), int(n[4]) ) ).reduceByKey(lambda v1,v2: v1 + v2).collect()\n",
    "\n",
    "[('GRIFFIN', 268),\n",
    " ('KALEB', 172),\n",
    " ('JOHNNY', 219),\n",
    " ('SAGE', 5),\n",
    " ('MIKE', 40),\n",
    " ('NAYELI', 44),\n",
    "....\n",
    "Formal API: reduceByKey(func: (V, V) ⇒ V): RDD[(K, V)]\n",
    "\n",
    "The above example was created from baby_names.csv file which was introduced in previous post What is Apache Spark?\n",
    "\n",
    "Back to Top\n",
    "\n",
    "\n",
    "aggregateByKey(zeroValue)(seqOp, combOp, [numTasks])\n",
    "Ok, I admit, this one drives me a bit nuts.  Why wouldn’t we just use reduceByKey?  I don’t feel smart enough to know when to use aggregateByKey over reduceByKey.  For example, the same results may be produced as reduceByKey:\n",
    "\n",
    ">>> filtered_rows = baby_names.filter(lambda line: \"Count\" not in line).map(lambda line: line.split(\",\"))\n",
    ">>> filtered_rows.map(lambda n:  (str(n[1]), int(n[4]) ) ).aggregateByKey(0, lambda k,v: int(v)+k, lambda v,k: k+v).collect()\n",
    "\n",
    "[('GRIFFIN', 268),\n",
    " ('KALEB', 172),\n",
    " ('JOHNNY', 219),\n",
    " ('SAGE', 5),\n",
    "...\n",
    "And again,  the above example was created from baby_names.csv file which was introduced in previous post What is Apache Spark?\n",
    "\n",
    "There’s a gist of aggregateByKey as well.\n",
    "\n",
    "Back to Top\n",
    "\n",
    "\n",
    "sortByKey(ascending=True, numPartitions=None, keyfunc=<function <lambda>>)\n",
    "This simply sorts the (K,V) pair by K.  Try it out. See examples above on where babyNames originates.\n",
    "\n",
    ">>> filtered_rows.map (lambda n:  (str(n[1]), int(n[4]) ) ).sortByKey().collect()\n",
    "[('AADEN', 18),\n",
    " ('AADEN', 11),\n",
    " ('AADEN', 10),\n",
    " ('AALIYAH', 50),\n",
    " ('AALIYAH', 44),\n",
    "...\n",
    "\n",
    "#opposite\n",
    ">>> filtered_rows.map (lambda n:  (str(n[1]), int(n[4]) ) ).sortByKey(False).collect()\n",
    "\n",
    "[('ZOIE', 5),\n",
    " ('ZOEY', 37),\n",
    " ('ZOEY', 32),\n",
    " ('ZOEY', 30),\n",
    "...\n",
    "Back to Top\n",
    "\n",
    "\n",
    "join(otherDataset, [numTasks])\n",
    "If you have relational database experience, this will be easy.  It’s joining of two datasets.  Other joins are available as well such as leftOuterJoin and rightOuterJoin.\n",
    "\n",
    ">>> names1 = sc.parallelize((\"abe\", \"abby\", \"apple\")).map(lambda a: (a, 1))\n",
    ">>> names2 = sc.parallelize((\"apple\", \"beatty\", \"beatrice\")).map(lambda a: (a, 1))\n",
    ">>> names1.join(names2).collect()\n",
    "\n",
    "[('apple', (1, 1))]\n",
    "leftOuterJoin, rightOuterJoin\n",
    "\n",
    ">>> names1.leftOuterJoin(names2).collect()\n",
    "[('abe', (1, None)), ('apple', (1, 1)), ('abby', (1, None))]\n",
    "\n",
    ">>> names1.rightOuterJoin(names2).collect()\n",
    "[('apple', (1, 1)), ('beatrice', (None, 1)), ('beatty', (None, 1))]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################Actions:\n",
    "Apache Spark Action Examples in Python\n",
    "As you may have learned in other apache spark tutorials on this site, action functions produce a computed value back to the Spark driver program.  This is unlike Transformations which produce RDDs, DataFrames or DataSets.  For example, an action function such as countwill produce a result back to the Spark driver.  These may seem easy at first but how actions are computed include some performance characteristic subtleties which you need to know.\n",
    "\n",
    "Actions trigger any previously constructed Spark transformations to be initialized.  Recall transformations are lazy initialized Spark data abstractions such as RDDs and DataFrames.  Any call to a Spark action will result in these data abstractions in the Spark directed acyclic graph to be evaluated.\n",
    "\n",
    "An ipython notebook file of all these examples is available in Reference section of this page.\n",
    "\n",
    "\n",
    "reduce\n",
    "collect\n",
    "count\n",
    "first\n",
    "take\n",
    "takeSample\n",
    "countByKey\n",
    "saveAsTextFile\n",
    "\n",
    "reduce(func)\n",
    "Aggregate the elements of a dataset through func\n",
    "\n",
    ">>> names1 = sc.parallelize([\"abe\", \"abby\", \"apple\"])\n",
    ">>> print names1.reduce(lambda t1, t2: t1+t2)\n",
    "abeabbyapple\n",
    "\n",
    ">>> names2 = sc.parallelize([\"apple\", \"beatty\", \"beatrice\"]).map(lambda a: [a, len(a)])\n",
    ">>> print names2.collect()\n",
    "[['apple', 5], ['beatty', 6], ['beatrice', 8]]\n",
    "\n",
    ">>> names2.flatMap(lambda t: [t[1]]).reduce(lambda t1, t2: t1+t2)\n",
    "19\n",
    "Back to Top\n",
    "\n",
    "\n",
    "collect(func)\n",
    "collect returns the elements of the RDD back to the driver program.\n",
    "\n",
    "collect is often used in previously provided examples such as Spark Transformation Examples in Python in order to show the values of the return.  Pyspark, for example, will print the values of the array back to the console.  This can be helpful in debugging programs.\n",
    "\n",
    "Examples\n",
    "\n",
    ">>> sc.parallelize([1,2,3]).flatMap(lambda x: [x,x,x]).collect()\n",
    "[1, 1, 1, 2, 2, 2, 3, 3, 3]\n",
    "Back to Top\n",
    "\n",
    "\n",
    "count()\n",
    "Number of elements in the RDD\n",
    "\n",
    ">>> names1 = sc.parallelize([\"abe\", \"abby\", \"apple\"])\n",
    ">>> names1.count()\n",
    "3\n",
    "Back to Top\n",
    "\n",
    "first()\n",
    "Return the first element in the RDD\n",
    "\n",
    ">>> names1 = sc.parallelize([\"abe\", \"abby\", \"apple\"])\n",
    ">>> names1.first()\n",
    "'abe'\n",
    "Back to Top\n",
    "\n",
    "take(n)\n",
    "Take the first n elements of the RDD.\n",
    "\n",
    "Works by first scanning one partition, and use the results from that partition to estimate the number of additional partitions needed to satisfy the limit.\n",
    "\n",
    "Translated from the Scala implementation in RDD#take().\n",
    "\n",
    "Can be much more convenient and economical to use take instead of collect to inspect a very large RDD\n",
    "\n",
    ">>> names1 = sc.parallelize([\"abe\", \"abby\", \"apple\"])\n",
    ">>> names1.take(2)\n",
    "['abe', 'abby']\n",
    "Back to Top\n",
    "\n",
    "\n",
    "takeSample(withReplacement, n, seed=None)\n",
    "Similar to take, in return size of n.  Includes boolean option  of with or without replacement and random generator seed which defaults to None\n",
    "\n",
    ">>> teams = sc.parallelize((\"twins\", \"brewers\", \"cubs\", \"white sox\", \"indians\", \"bad news bears\"))\n",
    ">>> teams.takeSample(True, 3)\n",
    "['brewers', 'brewers', 'twins']\n",
    "# run a few times to see different results\n",
    "Back to Top\n",
    "\n",
    "\n",
    "countByKey()\n",
    "Count the number of elements for each key, and return the result to the master as a dictionary.\n",
    "\n",
    ">>> hockeyTeams = sc.parallelize((\"wild\", \"blackhawks\", \"red wings\", \"wild\", \"oilers\", \"whalers\", \"jets\", \"wild\"))\n",
    ">>> hockeyTeams.map(lambda k: (k,1)).countByKey().items()\n",
    "[('red wings', 1),\n",
    " ('oilers', 1),\n",
    " ('blackhawks', 1),\n",
    " ('jets', 1),\n",
    " ('wild', 3),\n",
    " ('whalers', 1)]\n",
    "Back to Top\n",
    "\n",
    "\n",
    "saveAsTextFile(path, compressionCodecClass=None)\n",
    "Save RDD as text file, using string representations of elements.\n",
    "\n",
    "Parameters:\t\n",
    "path – path to file\n",
    "compressionCodecClass – (None by default) string i.e. “org.apache.hadoop.io.compress.GzipCodec”\n",
    ">>> hockeyTeams = sc.parallelize((\"wild\", \"blackhawks\", \"red wings\", \"wild\", \"oilers\", \"whalers\", \"jets\", \"wild\"))\n",
    ">>> hockeyTeams.map(lambda k: (k,1)).countByKey().items()\n",
    ">>> hockeyTeams.saveAsTextFile(\"hockey_teams.txt\")\n",
    "Produces:\n",
    "\n",
    "$ ls hockey_teams.txt/\n",
    "_SUCCESS\tpart-00001\tpart-00003\tpart-00005\tpart-00007\n",
    "part-00000\tpart-00002\tpart-00004\tpart-00006\n",
    "So, you’ll see each partition is written to it’s own file.  I have 8 partitions in dataset example here.\n",
    "\n",
    "Back to Top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
