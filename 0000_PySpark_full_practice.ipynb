{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1. Spark Session \n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Suri_PySpark_Practice\").config(\"Key\",\"value\").getOrCreate()\n",
    "#spark = SparkSession.builder.appName(\"Suri_PySpark_Practice\").config(conf=SparkConf()).getOrCreate()    --for default configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.app.name', '****R101100****'),\n",
       " ('spark.driver.memory', '4g'),\n",
       " ('spark.executor.memory', '4g'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.driver.port', '57853'),\n",
       " ('spark.app.id', 'local-1564077931574'),\n",
       " ('setMaster', 'local[2]'),\n",
       " ('Key', 'value'),\n",
       " ('spark.executor.cores', '4'),\n",
       " ('spark.cores.max', '4'),\n",
       " ('spark.sql.catalogImplementation', 'hive'),\n",
       " ('spark.driver.host', '192.168.76.1'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2A. Setting our own configurations. For multithreading use -->setMaster'='local[2]' \n",
    "conf = spark.sparkContext._conf.setAll([('spark.executor.memory', '4g'), ('spark.app.name', '****R101100****'), ('spark.executor.cores', '4'), ('spark.cores.max', '4'), ('spark.driver.memory','4g'), ('setMaster', 'local[2]')])\n",
    "#2B. Printing all used configurations    #sc._conf.getAll()\n",
    "sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create pandas DF , Spark DF , temp view \n",
    "dict1 = {\"Name\": [\"A\",\"B\",\"C\",\"D\",\"E\"], \"Age\":[20,21,23,20,22], \"Address\":[\"HYD\",\"Banglore\",\"Chennai\",\"Mumbai\",\"Banglore\"]}\n",
    "import pandas as pd\n",
    "df1 = pd.DataFrame(dict1)\n",
    "df2 = spark.createDataFrame(df1)\n",
    "df2.createOrReplaceTempView(\"DF2\")\n",
    "df3 = sqlContext.sql(\"select * from DF2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+--------+-------------+------------+\n",
      "|Name|Age| Address|Salary_double|Salary_float|\n",
      "+----+---+--------+-------------+------------+\n",
      "|   A| 20|     HYD|      20000.0|     20000.0|\n",
      "|   B| 21|Banglore|      21000.0|     21000.0|\n",
      "|   C| 23| Chennai|      23000.0|     23000.0|\n",
      "|   D| 20|  Mumbai|      20000.0|     20000.0|\n",
      "|   E| 22|Banglore|      22000.0|     22000.0|\n",
      "+----+---+--------+-------------+------------+\n",
      "\n",
      "+----+---+--------+\n",
      "|Name|Age| Address|\n",
      "+----+---+--------+\n",
      "|   A| 20|     HYD|\n",
      "|   B| 21|Banglore|\n",
      "|   C| 23| Chennai|\n",
      "|   D| 20|  Mumbai|\n",
      "|   E| 22|Banglore|\n",
      "+----+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df2.drop(\"Salary\")\n",
    "df2 = df2.withColumn(\"Salary_double\", (df2[\"Age\"]*1000).cast('double'))\n",
    "df2 = df2.withColumn(\"Salary_float\", (df2[\"Age\"]*1000).cast('float'))\n",
    "\n",
    "df2.show()\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+--------+------+\n",
      "|Name|Age| Address|Salary|\n",
      "+----+---+--------+------+\n",
      "|   A| 20|     HYD| 20000|\n",
      "|   B| 21|Banglore| 21000|\n",
      "|   C| 23| Chennai| 23000|\n",
      "|   D| 20|  Mumbai| 20000|\n",
      "|   E| 22|Banglore| 22000|\n",
      "+----+---+--------+------+\n",
      "\n",
      "+----+---+--------+\n",
      "|Name|Age| Address|\n",
      "+----+---+--------+\n",
      "|   A| 20|     HYD|\n",
      "|   B| 21|Banglore|\n",
      "|   C| 23| Chennai|\n",
      "|   D| 20|  Mumbai|\n",
      "|   E| 22|Banglore|\n",
      "+----+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df2.withColumn(\"Salary\", col(\"Age\")*1000)\n",
    "df2.show()\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++++++++ pandas ++++++++++++++\n",
      "['Categorical', 'CategoricalIndex', 'DataFrame', 'DateOffset', 'DatetimeIndex', 'ExcelFile', 'ExcelWriter', 'Expr', 'Float64Index', 'Grouper', 'HDFStore', 'Index', 'IndexSlice', 'Int64Index', 'Interval', 'IntervalIndex', 'MultiIndex', 'NaT', 'Panel', 'Period', 'PeriodIndex', 'RangeIndex', 'Series', 'SparseArray', 'SparseDataFrame', 'SparseSeries', 'Term', 'TimeGrouper', 'Timedelta', 'TimedeltaIndex', 'Timestamp', 'UInt64Index', 'WidePanel', '_DeprecatedModule', '__builtins__', '__cached__', '__doc__', '__docformat__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '__version__', '_hashtable', '_lib', '_libs', '_np_version_under1p10', '_np_version_under1p11', '_np_version_under1p12', '_np_version_under1p13', '_np_version_under1p14', '_np_version_under1p15', '_tslib', '_version', 'api', 'bdate_range', 'compat', 'concat', 'core', 'crosstab', 'cut', 'date_range', 'datetime', 'datetools', 'describe_option', 'errors', 'eval', 'factorize', 'get_dummies', 'get_option', 'get_store', 'groupby', 'infer_freq', 'interval_range', 'io', 'isna', 'isnull', 'json', 'lib', 'lreshape', 'match', 'melt', 'merge', 'merge_asof', 'merge_ordered', 'notna', 'notnull', 'np', 'offsets', 'option_context', 'options', 'pandas', 'parser', 'period_range', 'pivot', 'pivot_table', 'plot_params', 'plotting', 'pnow', 'qcut', 'read_clipboard', 'read_csv', 'read_excel', 'read_feather', 'read_fwf', 'read_gbq', 'read_hdf', 'read_html', 'read_json', 'read_msgpack', 'read_parquet', 'read_pickle', 'read_sas', 'read_sql', 'read_sql_query', 'read_sql_table', 'read_stata', 'read_table', 'reset_option', 'scatter_matrix', 'set_eng_float_format', 'set_option', 'show_versions', 'test', 'testing', 'timedelta_range', 'to_datetime', 'to_msgpack', 'to_numeric', 'to_pickle', 'to_timedelta', 'tools', 'tseries', 'tslib', 'unique', 'util', 'value_counts', 'wide_to_long']\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "++++++++++++++++++++++++++++++++++++ SparkSession ++++++++++++++\n",
      "['Builder', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__enter__', '__eq__', '__exit__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_convert_from_pandas', '_createFromLocal', '_createFromRDD', '_create_from_pandas_with_arrow', '_create_shell_session', '_get_numpy_record_dtype', '_inferSchema', '_inferSchemaFromList', '_instantiatedSession', '_repr_html_', 'builder', 'catalog', 'conf', 'createDataFrame', 'newSession', 'range', 'read', 'readStream', 'sparkContext', 'sql', 'stop', 'streams', 'table', 'udf', 'version']\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n"
     ]
    }
   ],
   "source": [
    "#Printing list of modules available in perticular package\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "print(\"++++++++++++++++++++++++++++++++++++ pandas ++++++++++++++\")\n",
    "print(dir(pd))\n",
    "print(\"++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "print(\"++++++++++++++++++++++++++++++++++++ SparkSession ++++++++++++++\")\n",
    "print(dir(SparkSession))\n",
    "print(\"++++++++++++++++++++++++++++++++++++++++++++++++++\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: long (nullable = true)\n",
      " |-- Address: string (nullable = true)\n",
      " |-- Salary: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read csv file\n",
    "rain_df = spark.read.csv(\"File2_Rain.csv\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(rain_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rain_df.show(2)\n",
    "rain_df = rain_df.withColumn(\"Date\", rain_df[\"Date\"].cast(\"date\"))\n",
    "#rain_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+----+----+----+----+----+-------+----+----+----+----+----+----+----+-------+\n",
      "|Date|RG01|RG02|RG03|RG04|RG05|RG07|RG08|RG09|RG10_30|RG11|RG12|RG14|RG15|RG16|RG17|RG18|RG20_25|\n",
      "+----+----+----+----+----+----+----+----+----+-------+----+----+----+----+----+----+----+-------+\n",
      "|null|2.43|3.36|2.88|2.48|0.78|2.49|2.57|2.93|   3.25|2.38|2.59|2.46|3.06|2.69|3.59|3.17|   3.15|\n",
      "|null|4.31| 1.4|5.46| 4.8|1.99|5.06|2.48|2.35|   6.48|4.95|5.71|3.57|5.77|3.28|5.77|6.02|    5.6|\n",
      "|null|6.55|7.35|5.84|6.48|7.57|4.47|7.39|7.31|   5.42|6.58|7.58|5.72|7.47|8.32|9.69|7.66|   7.17|\n",
      "|null|1.61|1.81| 1.7|1.49|1.11| 1.5|1.56|1.73|   1.18|1.37|1.47|1.33|1.19|1.21|1.52|1.09|   1.34|\n",
      "|null|5.01|5.88|3.12|5.01|5.09|5.15|5.14|5.01|   5.68|4.01|5.16|4.57| 5.5|5.61|5.62|5.49|   4.89|\n",
      "+----+----+----+----+----+----+----+----+----+-------+----+----+----+----+----+----+----+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- RG01: double (nullable = true)\n",
      " |-- RG02: double (nullable = true)\n",
      " |-- RG03: double (nullable = true)\n",
      " |-- RG04: double (nullable = true)\n",
      " |-- RG05: double (nullable = true)\n",
      " |-- RG07: double (nullable = true)\n",
      " |-- RG08: double (nullable = true)\n",
      " |-- RG09: double (nullable = true)\n",
      " |-- RG10_30: double (nullable = true)\n",
      " |-- RG11: double (nullable = true)\n",
      " |-- RG12: double (nullable = true)\n",
      " |-- RG14: double (nullable = true)\n",
      " |-- RG15: double (nullable = true)\n",
      " |-- RG16: double (nullable = true)\n",
      " |-- RG17: double (nullable = true)\n",
      " |-- RG18: double (nullable = true)\n",
      " |-- RG20_25: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rain_df.show(5)\n",
    "rain_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ruff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'toInt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-d19b1c444e03>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdf1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdict1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdf2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mdf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Salary\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtoInt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Age\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'toInt' is not defined"
     ]
    }
   ],
   "source": [
    "# Create pandas DF , Spark DF , temp view \n",
    "dict1 = {\"Name\": [\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\"], \"Age\":[20,21,23,20,22,19,24,21,20], \"Address\":[\"HYD\",\"Banglore\",\"Chennai\",\"Mumbai\",\"Banglore\",\"Mumbai\",\"Chennai\",\"Banglore\",\"HYD\"]}\n",
    "import pandas as pd\n",
    "df1 = pd.DataFrame(dict1)\n",
    "df2 = spark.createDataFrame(df1)\n",
    "df2=df2.withColumn(\"Salary\",toInt(col(\"Age\")*1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+-------+---+------+\n",
      "|Age_Address|Banglore|Chennai|HYD|Mumbai|\n",
      "+-----------+--------+-------+---+------+\n",
      "|         24|       0|      1|  0|     0|\n",
      "|         20|       0|      0|  2|     1|\n",
      "|         21|       2|      0|  0|     0|\n",
      "|         22|       1|      0|  0|     0|\n",
      "|         23|       0|      1|  0|     0|\n",
      "|         19|       0|      0|  0|     1|\n",
      "+-----------+--------+-------+---+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.crosstab(\"Age\",\"Address\").show()\n",
    "type(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.orderBy(\"Age\").agg({\"Salary\":\"F.sum\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: long (nullable = true)\n",
      " |-- Address: string (nullable = true)\n",
      " |-- Salary_double: double (nullable = true)\n",
      " |-- Salary_float: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.driver.memory', '4g'),\n",
       " ('spark.executor.memory', '4g'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.driver.port', '57853'),\n",
       " ('spark.app.id', 'local-1564077931574'),\n",
       " ('setMaster', 'local[2]'),\n",
       " ('Key', 'value'),\n",
       " ('spark.executor.cores', '4'),\n",
       " ('spark.cores.max', '4'),\n",
       " ('spark.app.name', 'Spark Updated Conf'),\n",
       " ('spark.sql.catalogImplementation', 'hive'),\n",
       " ('spark.driver.host', '192.168.76.1'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.getConf().getAll()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
